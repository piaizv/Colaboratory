{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP6OhaIO/TpYaBn79suUreQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/piaizv/Colaboratory/blob/main/scrapy_faces.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creación del proyecto"
      ],
      "metadata": {
        "id": "pf52ZoDEgHjE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JjL1SQFgDG5"
      },
      "outputs": [],
      "source": [
        "# instalación de Scrapy\n",
        "!pip install Scrapy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creación del proyecto\n",
        "!scrapy startproject project_faces scrapy"
      ],
      "metadata": {
        "id": "CbPN2xf_gU5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creación del spider\n",
        "!cd scrapy/project_faces && scrapy genspider FindFaces https://www.investigart.com/"
      ],
      "metadata": {
        "id": "ej1L2-6Bgnzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Edición de scrapy_generico/proyecto_generico/settings.py\n",
        "\n",
        "\n",
        "\n",
        "*   USER_AGENT\n",
        "*   ROBOTSTXT_OBEY\n",
        "*   DEFAULT_REQUEST_HEADERS\n",
        "\n"
      ],
      "metadata": {
        "id": "NCJ62iCGibB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile scrapy/project_faces/settings.py\n",
        "\n",
        "# Scrapy settings for project_faces project\n",
        "#\n",
        "# For simplicity, this file contains only settings considered important or\n",
        "# commonly used. You can find more settings consulting the documentation:\n",
        "#\n",
        "#     https://docs.scrapy.org/en/latest/topics/settings.html\n",
        "#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n",
        "#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n",
        "\n",
        "BOT_NAME = 'project_faces'\n",
        "\n",
        "SPIDER_MODULES = ['project_faces.spiders']\n",
        "NEWSPIDER_MODULE = 'project_faces.spiders'\n",
        "\n",
        "\n",
        "# Crawl responsibly by identifying yourself (and your website) on the user-agent\n",
        "#USER_AGENT = 'project_faces (+http://www.yourdomain.com)'\n",
        "USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36'\n",
        "\n",
        "# Obey robots.txt rules\n",
        "#ROBOTSTXT_OBEY = True\n",
        "ROBOTSTXT_OBEY = False\n",
        "\n",
        "# Configure maximum concurrent requests performed by Scrapy (default: 16)\n",
        "#CONCURRENT_REQUESTS = 32\n",
        "\n",
        "# Configure a delay for requests for the same website (default: 0)\n",
        "# See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay\n",
        "# See also autothrottle settings and docs\n",
        "#DOWNLOAD_DELAY = 3\n",
        "# The download delay setting will honor only one of:\n",
        "#CONCURRENT_REQUESTS_PER_DOMAIN = 16\n",
        "#CONCURRENT_REQUESTS_PER_IP = 16\n",
        "\n",
        "# Disable cookies (enabled by default)\n",
        "#COOKIES_ENABLED = False\n",
        "\n",
        "# Disable Telnet Console (enabled by default)\n",
        "#TELNETCONSOLE_ENABLED = False\n",
        "\n",
        "# Override the default request headers:\n",
        "#DEFAULT_REQUEST_HEADERS = {\n",
        "#   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
        "#   'Accept-Language': 'en',\n",
        "#}\n",
        "DEFAULT_REQUEST_HEADERS = {\n",
        "   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
        "   'Accept-Language': 'es',\n",
        "}\n",
        "\n",
        "# Enable or disable spider middlewares\n",
        "# See https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n",
        "#SPIDER_MIDDLEWARES = {\n",
        "#    'project_faces.middlewares.ProjectFacesSpiderMiddleware': 543,\n",
        "#}\n",
        "\n",
        "# Enable or disable downloader middlewares\n",
        "# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n",
        "#DOWNLOADER_MIDDLEWARES = {\n",
        "#    'project_faces.middlewares.ProjectFacesDownloaderMiddleware': 543,\n",
        "#}\n",
        "\n",
        "# Enable or disable extensions\n",
        "# See https://docs.scrapy.org/en/latest/topics/extensions.html\n",
        "#EXTENSIONS = {\n",
        "#    'scrapy.extensions.telnet.TelnetConsole': None,\n",
        "#}\n",
        "\n",
        "# Configure item pipelines\n",
        "# See https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n",
        "#ITEM_PIPELINES = {\n",
        "#    'project_faces.pipelines.ProjectFacesPipeline': 300,\n",
        "#}\n",
        "\n",
        "# Enable and configure the AutoThrottle extension (disabled by default)\n",
        "# See https://docs.scrapy.org/en/latest/topics/autothrottle.html\n",
        "#AUTOTHROTTLE_ENABLED = True\n",
        "# The initial download delay\n",
        "#AUTOTHROTTLE_START_DELAY = 5\n",
        "# The maximum download delay to be set in case of high latencies\n",
        "#AUTOTHROTTLE_MAX_DELAY = 60\n",
        "# The average number of requests Scrapy should be sending in parallel to\n",
        "# each remote server\n",
        "#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0\n",
        "# Enable showing throttling stats for every response received:\n",
        "#AUTOTHROTTLE_DEBUG = False\n",
        "\n",
        "# Enable and configure HTTP caching (disabled by default)\n",
        "# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings\n",
        "#HTTPCACHE_ENABLED = True\n",
        "#HTTPCACHE_EXPIRATION_SECS = 0\n",
        "#HTTPCACHE_DIR = 'httpcache'\n",
        "#HTTPCACHE_IGNORE_HTTP_CODES = []\n",
        "#HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'\n",
        "\n",
        "# Set settings whose default value is deprecated to a future-proof value\n",
        "REQUEST_FINGERPRINTER_IMPLEMENTATION = '2.7'\n",
        "TWISTED_REACTOR = 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'"
      ],
      "metadata": {
        "id": "7In0vQc-lJD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejecución del spider\n",
        "\n",
        "### No se debe ejecutar hasta que se haya implementado el spider."
      ],
      "metadata": {
        "id": "sJchdLIejozv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd scrapy/project_faces && scrapy crawl FindFaces"
      ],
      "metadata": {
        "id": "aXCHJ1RFBXsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd scrapy/project_faces && scrapy crawl FindFaces -o faces.json"
      ],
      "metadata": {
        "id": "OcOEA-_0j35Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementación del spider"
      ],
      "metadata": {
        "id": "Cq1PRakRkNaL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Versión inicial"
      ],
      "metadata": {
        "id": "xfymL2gXkWGJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile scrapy/project_faces/spiders/FindFaces.py\n",
        "import scrapy\n",
        "\n",
        "class FindfacesSpider(scrapy.Spider):\n",
        "    name = 'FindFaces'\n",
        "    allowed_domains = ['www.investigart.com']\n",
        "    start_urls = ['https://www.investigart.com/']\n",
        "\n",
        "    def parse(self, response):\n",
        "        pass"
      ],
      "metadata": {
        "id": "PEB06pi-kSfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Versión modificada"
      ],
      "metadata": {
        "id": "8E0AtX1ulM6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile scrapy/project_faces/spiders/FindFaces.py\n",
        "import scrapy\n",
        "\n",
        "class FindfacesSpider(scrapy.Spider):\n",
        "    name = 'FindFaces'\n",
        "    allowed_domains = ['www.investigart.com']\n",
        "    start_urls = ['https://www.investigart.com/']\n",
        "    pages = 1\n",
        "\n",
        "    def parse(self, response):\n",
        "      posts = response.css('.et_pb_salvattore_content > .et_pb_post')\n",
        "      print('numero de entradas:', len(posts))\n",
        "      for post in posts:\n",
        "        href = post.css('.et_pb_image_container > a::attr(href)').get()\n",
        "        print('href', href)\n",
        "        yield scrapy.Request(href, callback = self.parse_post, meta={'href': href})\n",
        "      next_page = response.css('.nextpostslink')\n",
        "      self.pages += 1\n",
        "      if next_page and self.pages < 4:\n",
        "        next_href = next_page.css('a::attr(href)').get()\n",
        "        yield scrapy.Request(next_href)\n",
        "\n",
        "    def parse_post(self, response):\n",
        "      href = response.meta.get('href')\n",
        "      element = response.xpath('//*[contains(@class, \"post-meta\")]/following-sibling::img')\n",
        "      img = element.css('img::attr(src)').get()\n",
        "      yield {\n",
        "        'href': href,\n",
        "        'img': img\n",
        "      }"
      ],
      "metadata": {
        "id": "RwIV7tTolQfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Procesando las caras de las imágenes"
      ],
      "metadata": {
        "id": "Ax7-CaQsnZDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from io import BytesIO\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import json\n",
        "import numpy as np\n",
        "import requests"
      ],
      "metadata": {
        "id": "mBgJljM0x0rN"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "json_file = 'scrapy/project_faces/faces.json'\n",
        "with open(json_file, 'r') as f:\n",
        "  data = json.load(f)\n",
        "\n",
        "images = []\n",
        "for item in data:\n",
        "  url = item['img']\n",
        "  img = Image.open(BytesIO(requests.get(url).content))\n",
        "  img = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n",
        "  images.append(img)"
      ],
      "metadata": {
        "id": "Im0QjPKxyFKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
        "faces_coords = []\n",
        "face_count = 0\n",
        "for img in images:\n",
        "  gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "  faces = faceCascade.detectMultiScale(\n",
        "        gray,\n",
        "        scaleFactor = 1.2,\n",
        "        minNeighbors = 8,\n",
        "        minSize = (30, 30)\n",
        "  )\n",
        "  faces_coords.append(faces)\n",
        "  print(\"Found {0} Faces!\".format(len(faces)))\n",
        "  face_count += len(faces)"
      ],
      "metadata": {
        "id": "SdB5t2j_y0ra"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}